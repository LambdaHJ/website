"use strict";(self.webpackChunkkoordinator_sh=self.webpackChunkkoordinator_sh||[]).push([[53],{1109:function(o){o.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docs":[{"type":"category","label":"Getting Started","collapsed":false,"items":[{"type":"link","label":"Introduction","href":"/docs/","docId":"introduction"},{"type":"link","label":"Installation","href":"/docs/installation","docId":"installation"}],"collapsible":true},{"type":"category","label":"Core Concepts","collapsed":false,"items":[{"type":"link","label":"Architecture","href":"/docs/core-concepts/architecture","docId":"core-concepts/architecture"},{"type":"link","label":"Resource Model","href":"/docs/core-concepts/resource-model","docId":"core-concepts/resource-model"},{"type":"link","label":"Priority","href":"/docs/core-concepts/priority","docId":"core-concepts/priority"},{"type":"link","label":"QoS","href":"/docs/core-concepts/qos","docId":"core-concepts/qos"}],"collapsible":true},{"type":"category","label":"User Manuals","collapsed":false,"items":[{"type":"link","label":"Colocation Profile","href":"/docs/user-manuals/colocation-profile","docId":"user-manuals/colocation-profile"}],"collapsible":true},{"type":"category","label":"Best Practices","collapsed":false,"items":[{"type":"link","label":"Colocation of Spark Jobs","href":"/docs/best-practices/colocation-of-spark-jobs","docId":"best-practices/colocation-of-spark-jobs"}],"collapsible":true}]},"docs":{"best-practices/colocation-of-spark-jobs":{"id":"best-practices/colocation-of-spark-jobs","title":"Colocation of Spark Jobs","description":"Apache Spark is an analysis engine for large-scale data processing, which is widely used in Big Data, SQL Analysis and Machine Learning scenarios. This tutorial provides a quick practice guide about running Spark jobs in colocation mode with other latency sensitive applications by Koordinator, which is helpful for imporving cluster resource utilization. For more details about how to use, compose, and work with Koordinator colocation, please refer to the Introdcution","sidebar":"docs"},"core-concepts/architecture":{"id":"core-concepts/architecture","title":"Architecture","description":"This topic describes the architecture, components, and core concepts associated with Koordinator deployments to Kubernetes. Koordinator consists of two control planes (Koordinator Scheduler/Koordinator Manager) and one daemonset component (Koordlet).","sidebar":"docs"},"core-concepts/priority":{"id":"core-concepts/priority","title":"Priority","description":"Koordinator defines a set of specifications on top of kubernetes priority class, and extends a dimension of priority to support fine-grained colocation.","sidebar":"docs"},"core-concepts/qos":{"id":"core-concepts/qos","title":"QoS","description":"QoS is used to express the running quality of the Pod on the node, such as the way to obtain resources, the proportion of resources obtained, and the QoS guarantee policy.","sidebar":"docs"},"core-concepts/resource-model":{"id":"core-concepts/resource-model","title":"Resource Model","description":"Colocation is a set of resource scheduling solutions for the fine grained orchestration of latency sensitive workloads with the big data computing workloads. Its core to solution two problems:","sidebar":"docs"},"installation":{"id":"installation","title":"Installation","description":"Since v0.1.0 (alpha/beta), Koordinator requires Kubernetes version >= 1.18.","sidebar":"docs"},"introduction":{"id":"introduction","title":"Introduction","description":"Welcome to Koordinator!","sidebar":"docs"},"user-manuals/colocation-profile":{"id":"user-manuals/colocation-profile","title":"Colocation Profile","description":"koord-manager has a variety of parameters that can be specified when creating a Custom Resource (CR). In this section, we will walk through all parameters in ClusterColocationProfile.","sidebar":"docs"}}}')}}]);